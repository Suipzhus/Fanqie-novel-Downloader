name: 在线下载小说 (仅TXT)

on:
  workflow_dispatch:  # 允许手动触发工作流
    inputs:
      novel_id:
        description: '小说ID (从番茄小说URL中获取)'
        required: true
      threads:
        description: '下载线程数 (1-10)'
        required: true
        default: '5'

# 添加必要的权限
permissions:
  contents: read  # 允许读取仓库内容
  actions: write  # 允许上传构建产物

jobs:
  download-novel:
    runs-on: ubuntu-latest
    steps:
      - name: 检出代码
        uses: actions/checkout@v3

      - name: 设置Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: 安装依赖
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # ebooklib 不再需要

      - name: 确保cookie.json存在
        run: |
          if [ ! -f "./cookie.json" ]; then
            echo '""' > "./cookie.json"
          fi

      - name: 安装虚拟显示服务 (如果脚本需要，否则可以移除)
        # 注意：原始脚本中并未实际使用xvfb，如果确定不需要可以移除此步骤
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb

      - name: 准备下载脚本
        run: |
          cat > download_novel.py << 'EOF'
          import sys
          import os
          import time
          import requests
          import bs4
          import re
          import json
          import random
          from concurrent.futures import ThreadPoolExecutor, as_completed
          from collections import OrderedDict
          # ebooklib 和 html 不再需要

          # 小说ID和保存路径
          novel_id = sys.argv[1]
          # output_format = sys.argv[2].lower() # 格式固定为 txt
          threads_count = int(sys.argv[2]) # 参数索引调整
          save_path = "novel_output"

          # EPUB生成函数 (已移除)
          # def generate_epub(...): ...

          # 确保输出目录存在
          os.makedirs(save_path, exist_ok=True)

          # 从GUI.py复制必要的函数和配置
          CONFIG = {
              "max_workers": threads_count,
              "max_retries": 3,
              "request_timeout": 15,
              "status_file": "chapter.json",
              "user_agents": [
                  "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
                  "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0",
                  "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
              ]
          }

          def get_headers(cookie=None):
              """生成随机请求头"""
              return {
                  "User-Agent": random.choice(CONFIG["user_agents"]),
                  "Cookie": cookie if cookie else get_cookie()
              }

          def get_cookie():
              """生成或加载Cookie"""
              cookie_path = "cookie.json"
              if os.path.exists(cookie_path):
                  try:
                      with open(cookie_path, 'r') as f:
                          return json.load(f)
                  except:
                      pass

              # 生成新Cookie
              for _ in range(10):
                  novel_web_id = random.randint(10**18, 10**19-1)
                  cookie = f'novel_web_id={novel_web_id}'
                  try:
                      resp = requests.get(
                          'https://fanqienovel.com',
                          headers={"User-Agent": random.choice(CONFIG["user_agents"])},
                          cookies={"novel_web_id": str(novel_web_id)},
                          timeout=10
                      )
                      if resp.ok:
                          with open(cookie_path, 'w') as f:
                              json.dump(cookie, f)
                          return cookie
                  except Exception as e:
                      print(f"Cookie生成失败: {str(e)}")
                      time.sleep(0.5)
              raise Exception("无法获取有效Cookie")

          def down_text(it):
              """下载章节内容"""
              max_retries = CONFIG.get('max_retries', 3)
              retry_count = 0
              content = ""

              while retry_count < max_retries:
                  try:
                      api_url = f"https://api.cenguigui.cn/api/tomato/content.php?item_id={it}"
                      response = requests.get(api_url, timeout=CONFIG["request_timeout"])
                      data = response.json()

                      if data.get("code") == 200:
                          content = data.get("data", {}).get("content", "")

                          # 移除HTML标签
                          content = re.sub(r'<header>.*?</header>', '', content, flags=re.DOTALL)
                          content = re.sub(r'<footer>.*?</footer>', '', content, flags=re.DOTALL)
                          content = re.sub(r'</?article>', '', content)
                          content = re.sub(r'<p idx="\d+">', '\n', content)
                          content = re.sub(r'</p>', '\n', content)
                          content = re.sub(r'<[^>]+>', '', content)
                          content = re.sub(r'\\u003c|\\u003e', '', content)

                          # 处理可能的重复章节标题行
                          title = data.get("data", {}).get("title", "")
                          if title and content.startswith(title):
                              content = content[len(title):].lstrip()

                          content = re.sub(r'\n{2,}', '\n', content).strip()
                          content = '\n'.join(['    ' + line if line.strip() else line for line in content.split('\n')])
                          break
                  except Exception as e:
                      print(f"请求失败: {str(e)}, 重试第{retry_count + 1}次...")
                      retry_count += 1
                      time.sleep(1 * retry_count)

              return content

          def get_book_info(book_id, headers):
              """获取书名、作者、简介"""
              url = f'https://fanqienovel.com/page/{book_id}'
              response = requests.get(url, headers=headers)
              if response.status_code != 200:
                  print(f"网络请求失败，状态码: {response.status_code}")
                  return None, None, None

              soup = bs4.BeautifulSoup(response.text, 'html.parser')

              # 获取书名
              name_element = soup.find('h1')
              name = name_element.text if name_element else "未知书名"

              # 获取作者
              author_name_element = soup.find('div', class_='author-name')
              author_name = None
              if author_name_element:
                  author_name_span = author_name_element.find('span', class_='author-name-text')
                  author_name = author_name_span.text if author_name_span else "未知作者"

              # 获取简介
              description_element = soup.find('div', class_='page-abstract-content')
              description = None
              if description_element:
                  description_p = description_element.find('p')
                  description = description_p.text if description_p else "无简介"

              return name, author_name, description

          def extract_chapters(soup):
              """解析章节列表"""
              chapters = []
              for idx, item in enumerate(soup.select('div.chapter-item')):
                  a_tag = item.find('a')
                  if not a_tag:
                      continue

                  raw_title = a_tag.get_text(strip=True)

                  # 特殊章节
                  if re.match(r'^(番外|特别篇|if线)\s*', raw_title):
                      final_title = raw_title
                  else:
                      clean_title = re.sub(
                          r'^第[一二三四五六七八九十百千\d]+章\s*',
                          '',
                          raw_title
                      ).strip()
                      final_title = f"第{idx+1}章 {clean_title}"

                  chapters.append({
                      "id": a_tag['href'].split('/')[-1],
                      "title": final_title,
                      "url": f"https://fanqienovel.com{a_tag['href']}",
                      "index": idx
                  })

              return chapters

          def download_novel(book_id, save_path):
              """下载小说的主函数"""
              try:
                  headers = get_headers()
                  print("正在获取书籍信息...")

                  # 获取书籍信息
                  name, author_name, description = get_book_info(book_id, headers)
                  if not name:
                      raise Exception("无法获取书籍信息，请检查小说ID或网络连接")

                  print(f"书名：《{name}》")
                  print(f"作者：{author_name}")
                  print(f"简介：{description}")

                  # 获取章节列表
                  url = f'https://fanqienovel.com/page/{book_id}'
                  response = requests.get(url, headers=headers)
                  soup = bs4.BeautifulSoup(response.text, 'html.parser')

                  chapters = extract_chapters(soup)
                  if not chapters:
                      raise Exception("未找到任何章节")

                  print(f"\n开始下载，共 {len(chapters)} 章")
                  os.makedirs(save_path, exist_ok=True)

                  # 创建文件并写入信息
                  output_file = os.path.join(save_path, f"{name}.txt")
                  with open(output_file, 'w', encoding='utf-8') as f:
                      f.write(f"书名：《{name}》\n作者：{author_name}\n\n简介：\n{description}\n\n")

                  # 不再需要 EPUB 相关逻辑
                  # if output_format == 'epub': ...

                  # 下载章节
                  total_chapters = len(chapters)
                  success_count = 0
                  downloaded_chapters = set()
                  content_cache = OrderedDict()

                  # 先顺序下载前5章
                  for chapter in chapters[:5]:
                      content = down_text(chapter["id"])
                      if content:
                          content_cache[chapter["index"]] = (chapter, content)
                          downloaded_chapters.add(chapter["id"])
                
